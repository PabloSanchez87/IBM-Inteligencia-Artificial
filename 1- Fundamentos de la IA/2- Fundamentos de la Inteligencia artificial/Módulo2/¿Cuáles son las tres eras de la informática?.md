# ¿Cuáles son las tres eras de la informática?

## La era de la tabulación

`Las personas han analizado datos durante siglos`

Durante siglos, la gente ha luchado por comprender el significado que se oculta en grandes cantidades de datos. Al fin y al cabo, una cosa es calcular cuántos árboles crecen en un millón de kilómetros cuadrados de bosque. Y otra cosa es clasificar qué especies de árboles son, cómo se agrupan a distintas altitudes y qué se podría construir con la madera que proporcionan. Esta información puede ser difícil de extraer de una gran cantidad de datos. Como es difícil verlos sin ayuda, los científicos los llaman **datos oscuros**. Es información sin estructura: un enorme revoltijo de hechos sin clasificar.

Para clasificar los datos no estructurados, los humanos han creado muchas máquinas de calcular diferentes. Hace más de 2.000 años, los recaudadores de impuestos del emperador Qin Shihuang utilizaban el ábaco -un dispositivo con cuentas sobre alambres- para desglosar los recibos de impuestos y ordenarlos por categorías. A partir de ahí, pudieron calcular cuánto debía gastar el Emperador en construir ampliaciones de la Gran Muralla China.

En Inglaterra, a mediados del siglo XIX, Charles Babbage y Ada Lovelace diseñaron (pero nunca terminaron) lo que llamaron una “máquina diferencial”, concebida para realizar cálculos complejos con logaritmos y trigonometría. Si la hubieran construido, la máquina diferencial podría haber ayudado a la Armada inglesa a elaborar tablas de mareas oceánicas y sondeos de profundidad que pudieran guiar a los marineros ingleses en aguas bravas.

A principios del siglo XX, empresas como IBM utilizaban máquinas para tabular y analizar las cifras del censo de poblaciones nacionales enteras. No solo contaban personas. Encontraron patrones y estructuras en los datos, un significado práctico más allá de los meros números. Estas máquinas descubrieron la forma en que los distintos grupos de población se desplazaban y asentaban, se ganaban la vida o sufrían problemas de salud, información que ayudó a los gobiernos a comprenderlos mejor y atenderlos mejor.

La palabra que hay que recordar a lo largo de esos veinte siglos es **tabular**. La tabulación consiste en “cortar y trocear” los datos para estructurarlos y descubrir patrones de información útil. Se tabula cuando se quiere tener una idea de lo que significan realmente todas esas columnas y filas de datos de una tabla.
 
Los investigadores llaman a estos siglos la `Era de la tabulación`, una época en la que las máquinas ayudaban a los humanos a ordenar los datos en estructuras para revelar sus secretos.

## La era de la programación

`El análisis de datos cambió en los años 40`

Durante la agitación de la Segunda Guerra Mundial, surgió un nuevo enfoque de los datos oscuros: la `era de la programación`. Los científicos empezaron a construir ordenadores electrónicos, como el Electronic Numerical Integrator and Computer (ENIAC) de la Universidad de Pensilvania, que podían ejecutar más de un tipo de instrucción (que hoy llamamos “programas”) para realizar más de un tipo de cálculo. ENIAC, por ejemplo, no solo calculaba tablas de tiro de artillería para el ejército estadounidense, sino que trabajó en secreto para estudiar la viabilidad de las armas termonucleares.

Esto supuso un gran avance. Los ordenadores programables guiaron a los astronautas de la Tierra a la Luna y fueron reprogramados durante la accidentada misión del Apolo 13 para traer a sus astronautas de vuelta a la Tierra sanos y salvos.

Usted ha crecido durante la era de la programación. Maneja incluso el teléfono que tiene en la mano. Pero el problema de los datos oscuros también ha crecido. Las empresas y la tecnología modernas generan tantos datos que ni el superordenador más programable podría analizarlos antes de la “muerte por calor” del universo. La informática electrónica se enfrenta a una crisis.

## La era de la IA

![alt text](/resources/historia.png)

### La era de la IA comenzó el verano de 1956

A principios del verano de 1956, un pequeño grupo de investigadores, encabezados por John McCarthy y Marvin Minsky, se reunieron en la Universidad de Dartmouth de New Hampshire. Allí, en una de las universidades más antiguas de Estados Unidos, lanzaron una revolución en la investigación cientíﬁca y acuñaron el término "inteligencia artiﬁcial".

Los investigadores propusieron que "cada aspecto del aprendizaje o cualquier otra característica de la inteligencia puede describirse con tanta precisión que se puede lograr que una máquina lo simule". Llamaron a su teoría "inteligencia artiﬁcial" y recaudaron millones de dólares para conseguirla en 20 años. Durante las dos décadas siguientes consiguieron grandes logros: crearon máquinas capaces de demostrar teoremas geométricos, hablar inglés sencillo e incluso resolver problemas de álgebra. 

Durante un breve periodo de tiempo, la IA fue uno de los campos más apasionantes de la informática.

### Pero llegó el invierno
A principios de la década de 1970, quedó claro que el problema era mayor de lo que los investigadores imaginaban. Había límites fundamentales que ninguna cantidad de dinero y esfuerzo podía resolver.

- Potencia de cálculo limitada

    Hoy en día, es importante que un ordenador tenga suficiente capacidad de procesamiento y memoria. Todos los anuncios de empresas como Apple o Dell destacan la velocidad de sus procesadores y la cantidad de datos que pueden gestionar. Pero en 1976, los científicos se dieron cuenta de que incluso los ordenadores más populares de la época, que trabajaban con lenguaje natural, solo podían manipular un vocabulario de unas 20 palabras. Un trabajo como igualar el funcionamiento de la retina humana podría requerir millones de instrucciones por segundo, en una época en la que el ordenador más rápido del mundo solo podía ejecutar unas cien. A principios de la década de 1970, quedó claro que el problema era mayor de lo que los investigadores imaginaban. Había límites fundamentales que ninguna cantidad de dinero y esfuerzo podía resolver.

- Almacenamiento de datos limitado

    Incluso un razonamiento sencillo y de sentido común requiere mucha información que lo respalde. Pero en 1970 nadie sabía cómo construir una base de datos lo suficientemente grande como para contener siquiera la información que conoce un niño de 2 años.

### Las condiciones fueron duras durante medio siglo
La tecnología y la teoría de la IA tardaron aproximadamente una década en ponerse al día, principalmente con nuevas formas de IA denominadas "sistemas expertos". Se limitaban a conocimientos específicos que podían manipularse con conjuntos de reglas. Funcionaron bien durante un tiempo y se popularizaron en los años ochenta. El dinero entraba a raudales. Los investigadores invirtieron en enormes máquinas mainframe que costaban millones de dólares y ocupaban plantas enteras de grandes edificios universitarios y corporativos. Parecía que la IA volvía a estar en auge.

Pero pronto las necesidades de científicos, empresas y gobiernos superaron incluso a estos nuevos sistemas. Una vez más, la financiación de la IA se desvaneció.

### Luego vino otro enfriamiento de la IA

A finales de la década de 1980, el auge de la investigación en IA se enfrió, en parte, por el auge de los ordenadores personales. Las máquinas de Apple e IBM, instaladas en los escritorios de los hogares, eran más potentes que los enormes sistemas corporativos adquiridos pocos años antes. Las empresas y los gobiernos dejaron de invertir en investigación informática a gran escala y la financiación se agotó.

Más de 300 empresas de IA cerraron o quebraron durante el **Segundo Invierno de la IA.**

### Ahora, el pronóstico es soleado

A mediados de la década de 1990, casi medio siglo después del proyecto de investigación de Dartmouth, el Segundo invierno de la IA comenzó a descongelarse. Entre bastidores, el procesamiento informático alcanzó por fin velocidades suficientes para que las máquinas pudieran resolver problemas complejos.

Paralelamente, la gente empezó a ver la capacidad de la IA para participar en juegos sofisticados.

- En 1997, el Deep Blue de IBM venció al campeón mundial de ajedrez procesando más de 200 millones de posibles jugadas por segundo.
- En 2005, un robot de la Universidad de Stanford se guió a sí mismo por un trayecto desértico de 131 millas.
- En 2011, Watson de IBM derrotó a dos grandes campeones en el juego Jeopardy!

En la actualidad, la IA ha demostrado su capacidad en campos que van desde la investigación del cáncer y el análisis de grandes volúmenes de datos hasta los sistemas de defensa y la producción de energía. La inteligencia artificial ha alcanzado la mayoría de edad. La IA se ha convertido en uno de los campos más candentes de la informática. Sus logros repercuten cada día en las personas y sus capacidades aumentan exponencialmente. Los dos Inviernos de la IA han llegado a su fin.